"""
============================================================
PIPELINE COMPLETO: CLUSTERING ‚Üí DELIMITACI√ìN ‚Üí ESTIMACI√ìN
============================================================

Este script implementa el flujo completo de:
1. Clusterizaci√≥n espacial de datos
2. Delimitaci√≥n de dominios (interpolaci√≥n de clusters)
3. Estimaci√≥n con KNN por dominio

Autor: Sistema
Fecha: 2024
"""

#%%
# ============================================================
# PASO 1: CONFIGURACI√ìN E IMPORTS
# ============================================================
import sys
sys.path.append('../')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from src.clustering import ClusterKmeans
from src.estimacion import EstimadorEspacial
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

print("="*70)
print("üöÄ PIPELINE DE ESTIMACI√ìN ESPACIAL")
print("="*70)
print("‚úÖ Librer√≠as cargadas correctamente\n")

#%%
# ============================================================
# PASO 2: CARGA DE DATOS
# ============================================================
print("\n" + "="*70)
print("üìÇ PASO 1: CARGA DE DATOS")
print("="*70)

df = pd.read_csv("../data/raw/bd_dm_cmp_entry.csv", sep=";")
columnas = ["midx", "midy", "midz", "starkey_min"]
df = df[columnas].copy()

# Extraer variables
x = df['midx'].values
z = df['midz'].values
atributo = df['starkey_min'].values

print(f"\nüìä Datos cargados exitosamente:")
print(f"   ‚Ä¢ Total de puntos: {len(df)}")
print(f"   ‚Ä¢ Coordenadas X: [{x.min():.1f}, {x.max():.1f}]")
print(f"   ‚Ä¢ Coordenadas Z: [{z.min():.1f}, {z.max():.1f}]")
print(f"   ‚Ä¢ Atributo (starkey_min):")
print(f"      - Media: {atributo.mean():.2f}")
print(f"      - Std: {atributo.std():.2f}")
print(f"      - Min: {atributo.min():.2f}")
print(f"      - Max: {atributo.max():.2f}")

# Visualizaci√≥n inicial
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Panel 1: Distribuci√≥n espacial
ax = axes[0]
scatter = ax.scatter(x, z, c=atributo, cmap='RdYlBu_r',
                    s=50, alpha=0.7, edgecolors='k', linewidth=0.5)
ax.set_title('Distribuci√≥n Espacial del Atributo', fontweight='bold', fontsize=14)
ax.set_xlabel('X (midx)', fontsize=12)
ax.set_ylabel('Z (midz)', fontsize=12)
ax.grid(alpha=0.3)
plt.colorbar(scatter, ax=ax, label='starkey_min')

# Panel 2: Histograma del atributo
ax = axes[1]
ax.hist(atributo, bins=50, alpha=0.7, color='steelblue', edgecolor='k')
ax.axvline(atributo.mean(), color='red', linestyle='--', linewidth=2,
          label=f'Media: {atributo.mean():.2f}')
ax.set_title('Distribuci√≥n del Atributo', fontweight='bold', fontsize=14)
ax.set_xlabel('starkey_min', fontsize=12)
ax.set_ylabel('Frecuencia', fontsize=12)
ax.legend()
ax.grid(alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

#%%
# ============================================================
# PASO 3: CLUSTERIZACI√ìN
# ============================================================
print("\n" + "="*70)
print("üîµ PASO 2: CLUSTERIZACI√ìN ESPACIAL")
print("="*70)

print("\nüìå ¬øQu√© hace la clusterizaci√≥n?")
print("   La clusterizaci√≥n agrupa los datos en dominios homog√©neos")
print("   considerando tanto la ubicaci√≥n espacial como el atributo.")
print("   Usamos K-means ponderado que balancea:")
print("   ‚Ä¢ Proximidad espacial (coordenadas X, Z)")
print("   ‚Ä¢ Similitud en el atributo (starkey_min)")

# Par√°metros de clustering
n_clusters = 5
w_spatial = 0.65  # 65% peso espacial, 35% peso de atributo

print(f"\n‚öôÔ∏è  Par√°metros:")
print(f"   ‚Ä¢ N√∫mero de clusters: {n_clusters}")
print(f"   ‚Ä¢ Peso espacial: {w_spatial} (65% espacio, 35% atributo)")

# Crear y entrenar el clusterer
clusterer = ClusterKmeans(n_clusters=n_clusters, w_spatial=w_spatial)
clusterer.fit(x, z, atributo)

print("\n‚úÖ Clustering completado")
print(f"   Se han identificado {n_clusters} dominios")

# Obtener estad√≠sticas
stats = clusterer.get_stats()
print("\nüìä Estad√≠sticas por cluster:")
for i, stat in stats.items():
    print(f"\n   Cluster {i}:")
    print(f"      ‚Ä¢ Puntos: {stat['n_points']}")
    print(f"      ‚Ä¢ Media: {stat['mean']:.2f}")
    print(f"      ‚Ä¢ Std: {stat['std']:.2f}")
    print(f"      ‚Ä¢ CV: {stat['efecto_proporcional']:.3f}")

# Visualizaci√≥n de clusters
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Panel 1: Clusters espaciales
ax = axes[0]
scatter = ax.scatter(x, z, c=clusterer.clusters, cmap='viridis',
                    s=50, alpha=0.7, edgecolors='k', linewidth=0.5)
ax.set_title(f'Clusters Espaciales (k={n_clusters}, w={w_spatial})',
            fontweight='bold', fontsize=14)
ax.set_xlabel('X (midx)', fontsize=12)
ax.set_ylabel('Z (midz)', fontsize=12)
ax.grid(alpha=0.3)
plt.colorbar(scatter, ax=ax, label='Cluster')

# Panel 2: Comparaci√≥n con atributo original
ax = axes[1]
scatter = ax.scatter(x, z, c=atributo, cmap='RdYlBu_r',
                    s=50, alpha=0.7, edgecolors='k', linewidth=0.5)
ax.set_title('Atributo Original', fontweight='bold', fontsize=14)
ax.set_xlabel('X (midx)', fontsize=12)
ax.set_ylabel('Z (midz)', fontsize=12)
ax.grid(alpha=0.3)
plt.colorbar(scatter, ax=ax, label='starkey_min')

plt.tight_layout()
plt.show()

# M√©tricas globales
metricas = clusterer.get_global_metrics()
print(f"\nüìà M√©tricas globales:")
print(f"   ‚Ä¢ Std promedio: {metricas['std_prom']:.2f}")
print(f"   ‚Ä¢ CV promedio: {metricas['cv_prom']:.3f}")

#%%
# ============================================================
# PASO 4: DIVISI√ìN TRAIN/TEST
# ============================================================
print("\n" + "="*70)
print("üîÄ PASO 3: DIVISI√ìN TRAIN/TEST")
print("="*70)

print("\nüìå ¬øPor qu√© dividir los datos?")
print("   Para evaluar el desempe√±o del modelo de estimaci√≥n necesitamos:")
print("   ‚Ä¢ 80% de datos para ENTRENAR el modelo")
print("   ‚Ä¢ 20% de datos para PROBAR el modelo (nunca vistos)")

# Divisi√≥n estratificada por cluster
indices = np.arange(len(x))
train_idx, test_idx = train_test_split(
    indices,
    test_size=0.2,
    random_state=42,
    stratify=clusterer.clusters  # Mantener proporciones de clusters
)

# Extraer train y test
x_train, x_test = x[train_idx], x[test_idx]
z_train, z_test = z[train_idx], z[test_idx]
attr_train, attr_test = atributo[train_idx], atributo[test_idx]
clusters_train, clusters_test = clusterer.clusters[train_idx], clusterer.clusters[test_idx]

print(f"\nüìä Divisi√≥n completada:")
print(f"   ‚Ä¢ Datos de entrenamiento: {len(x_train)} puntos ({len(x_train)/len(x)*100:.1f}%)")
print(f"   ‚Ä¢ Datos de test: {len(x_test)} puntos ({len(x_test)/len(x)*100:.1f}%)")

# Verificar distribuci√≥n de clusters
print(f"\nüìä Distribuci√≥n por cluster:")
for i in range(n_clusters):
    n_train = np.sum(clusters_train == i)
    n_test = np.sum(clusters_test == i)
    total = n_train + n_test
    print(f"   Cluster {i}: {n_train} train ({n_train/total*100:.1f}%), "
          f"{n_test} test ({n_test/total*100:.1f}%)")

# Visualizaci√≥n de la divisi√≥n
fig, ax = plt.subplots(figsize=(12, 8))
ax.scatter(x_train, z_train, c='blue', s=30, alpha=0.6,
          label=f'Train ({len(x_train)} pts)', edgecolors='k', linewidth=0.3)
ax.scatter(x_test, z_test, c='red', s=80, alpha=0.8, marker='s',
          label=f'Test ({len(x_test)} pts)', edgecolors='k', linewidth=0.5)
ax.set_title('Divisi√≥n Train/Test', fontweight='bold', fontsize=14)
ax.set_xlabel('X (midx)', fontsize=12)
ax.set_ylabel('Z (midz)', fontsize=12)
ax.legend(fontsize=12)
ax.grid(alpha=0.3)
plt.tight_layout()
plt.show()

#%%
# ============================================================
# PASO 5: ESTIMACI√ìN GLOBAL CON KNN
# ============================================================
print("\n" + "="*70)
print("üéØ PASO 4: ESTIMACI√ìN GLOBAL CON KNN")
print("="*70)

print("\nüìå ¬øQu√© hace KNN?")
print("   K-Nearest Neighbors (KNN) estima el valor de un punto desconocido")
print("   usando el promedio ponderado de sus K vecinos m√°s cercanos.")
print("   ‚Ä¢ Ventaja: Simple, r√°pido, no asume distribuci√≥n estad√≠stica")
print("   ‚Ä¢ Desventaja: Sensible a outliers y no genera incertidumbre")

# Par√°metros KNN
n_neighbors = 10
print(f"\n‚öôÔ∏è  Par√°metros:")
print(f"   ‚Ä¢ N√∫mero de vecinos (k): {n_neighbors}")
print(f"   ‚Ä¢ Ponderaci√≥n: Por distancia (vecinos cercanos pesan m√°s)")

# Crear y entrenar estimador KNN
estimador_knn = EstimadorEspacial(metodo='knn', n_neighbors=n_neighbors)
estimador_knn.fit(x_train, z_train, attr_train)

print(f"\n‚úÖ Modelo KNN entrenado con {len(x_train)} puntos")

# Predecir en conjunto de test
print("\nüîÆ Realizando predicciones en conjunto de test...")
pred_knn = estimador_knn.predict(x_test, z_test)

# Calcular m√©tricas
mae_knn = mean_absolute_error(attr_test, pred_knn)
rmse_knn = np.sqrt(mean_squared_error(attr_test, pred_knn))
r2_knn = r2_score(attr_test, pred_knn)

print("\nüìä M√âTRICAS EN DATOS DE TEST:")
print(f"   ‚Ä¢ MAE (Error Absoluto Medio): {mae_knn:.3f}")
print(f"     ‚Üí En promedio, nos equivocamos en {mae_knn:.3f} unidades")
print(f"   ‚Ä¢ RMSE (Ra√≠z del Error Cuadr√°tico Medio): {rmse_knn:.3f}")
print(f"     ‚Üí Penaliza m√°s los errores grandes")
print(f"   ‚Ä¢ R¬≤ (Coeficiente de Determinaci√≥n): {r2_knn:.3f}")
print(f"     ‚Üí El modelo explica {r2_knn*100:.1f}% de la varianza")

# Visualizaci√≥n de resultados
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Panel 1: Predicho vs Real
ax = axes[0]
ax.scatter(attr_test, pred_knn, alpha=0.6, s=60, edgecolors='k', linewidth=0.5)
min_val = min(attr_test.min(), pred_knn.min())
max_val = max(attr_test.max(), pred_knn.max())
ax.plot([min_val, max_val], [min_val, max_val],
       'r--', linewidth=2, label='Predicci√≥n perfecta')
ax.set_title(f'KNN: Predicho vs Real\nMAE={mae_knn:.2f}, RMSE={rmse_knn:.2f}, R¬≤={r2_knn:.3f}',
            fontweight='bold', fontsize=13)
ax.set_xlabel('Valor Real', fontsize=12)
ax.set_ylabel('Valor Predicho', fontsize=12)
ax.legend(fontsize=11)
ax.grid(alpha=0.3)
ax.set_aspect('equal', adjustable='box')

# Panel 2: Distribuci√≥n de errores
ax = axes[1]
errores = attr_test - pred_knn
ax.hist(errores, bins=30, alpha=0.7, color='steelblue', edgecolor='k')
ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Error = 0')
ax.axvline(errores.mean(), color='orange', linestyle='--', linewidth=2,
          label=f'Media = {errores.mean():.2f}')
ax.set_title('Distribuci√≥n de Errores', fontweight='bold', fontsize=13)
ax.set_xlabel('Error (Real - Predicho)', fontsize=12)
ax.set_ylabel('Frecuencia', fontsize=12)
ax.legend(fontsize=11)
ax.grid(alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

#%%
# ============================================================
# PASO 6: ESTIMACI√ìN POR CLUSTER
# ============================================================
print("\n" + "="*70)
print("üéØ PASO 5: ESTIMACI√ìN POR DOMINIO (CLUSTER)")
print("="*70)

print("\nüìå ¬øPor qu√© estimar por cluster?")
print("   Cada cluster tiene caracter√≠sticas propias (media, std diferentes).")
print("   Al estimar dentro de cada dominio:")
print("   ‚Ä¢ Mejoramos la precisi√≥n local")
print("   ‚Ä¢ Respetamos la homogeneidad de cada dominio")
print("   ‚Ä¢ Evitamos contaminar estimaciones entre dominios distintos")

# Resultados por cluster
resultados_clusters = {}

print(f"\nüîß Estimando en {n_clusters} clusters...")

for cluster_id in range(n_clusters):
    print(f"\n{'‚îÄ'*60}")
    print(f"üìç CLUSTER {cluster_id}")
    print(f"{'‚îÄ'*60}")
    
    # Filtrar datos de este cluster
    mask_train = clusters_train == cluster_id
    mask_test = clusters_test == cluster_id
    
    x_train_c = x_train[mask_train]
    z_train_c = z_train[mask_train]
    attr_train_c = attr_train[mask_train]
    
    x_test_c = x_test[mask_test]
    z_test_c = z_test[mask_test]
    attr_test_c = attr_test[mask_test]
    
    print(f"   ‚Ä¢ Puntos de entrenamiento: {len(x_train_c)}")
    print(f"   ‚Ä¢ Puntos de test: {len(x_test_c)}")
    
    # Solo estimar si hay suficientes datos
    if len(x_test_c) == 0:
        print(f"   ‚ö†Ô∏è  Sin datos de test en este cluster, saltando...")
        continue
    
    if len(x_train_c) < n_neighbors:
        print(f"   ‚ö†Ô∏è  Pocos datos de entrenamiento ({len(x_train_c)} < {n_neighbors})")
        print(f"      Ajustando k a {len(x_train_c)}")
        k_actual = len(x_train_c)
    else:
        k_actual = n_neighbors
    
    # Entrenar estimador para este cluster
    estimador_c = EstimadorEspacial(metodo='knn', n_neighbors=k_actual)
    estimador_c.fit(x_train_c, z_train_c, attr_train_c)
    
    # Predecir
    pred_c = estimador_c.predict(x_test_c, z_test_c)
    
    # M√©tricas
    mae_c = mean_absolute_error(attr_test_c, pred_c)
    rmse_c = np.sqrt(mean_squared_error(attr_test_c, pred_c))
    r2_c = r2_score(attr_test_c, pred_c)
    
    print(f"   ‚Ä¢ MAE:  {mae_c:.3f}")
    print(f"   ‚Ä¢ RMSE: {rmse_c:.3f}")
    print(f"   ‚Ä¢ R¬≤:   {r2_c:.3f}")
    
    # Guardar resultados
    resultados_clusters[cluster_id] = {
        'n_train': len(x_train_c),
        'n_test': len(x_test_c),
        'mae': mae_c,
        'rmse': rmse_c,
        'r2': r2_c,
        'predicciones': pred_c,
        'reales': attr_test_c
    }

#%%
# ============================================================
# PASO 7: COMPARACI√ìN GLOBAL VS POR CLUSTER
# ============================================================
print("\n" + "="*70)
print("üìä PASO 6: COMPARACI√ìN DE ENFOQUES")
print("="*70)

print("\nüîç Comparando dos estrategias:")
print("   1. Estimaci√≥n GLOBAL: Un solo modelo KNN para todos los datos")
print("   2. Estimaci√≥n POR CLUSTER: Un modelo KNN independiente por dominio")

# Calcular m√©tricas promedio ponderado por cluster
maes_cluster = []
rmses_cluster = []
r2s_cluster = []
pesos = []

for cluster_id, resultado in resultados_clusters.items():
    if 'mae' in resultado:
        maes_cluster.append(resultado['mae'])
        rmses_cluster.append(resultado['rmse'])
        r2s_cluster.append(resultado['r2'])
        pesos.append(resultado['n_test'])

# Promedios ponderados
total_test = sum(pesos)
mae_clusters_prom = np.average(maes_cluster, weights=pesos)
rmse_clusters_prom = np.average(rmses_cluster, weights=pesos)
r2_clusters_prom = np.average(r2s_cluster, weights=pesos)

# Tabla comparativa
print("\n" + "="*70)
print("üìã TABLA COMPARATIVA")
print("="*70)

comparacion = pd.DataFrame({
    'M√©todo': ['Global (1 modelo)', 'Por Cluster (5 modelos)'],
    'MAE': [mae_knn, mae_clusters_prom],
    'RMSE': [rmse_knn, rmse_clusters_prom],
    'R¬≤': [r2_knn, r2_clusters_prom]
})

print("\n" + comparacion.to_string(index=False))

# Calcular mejoras
mejora_mae = ((mae_knn - mae_clusters_prom) / mae_knn) * 100
mejora_rmse = ((rmse_knn - rmse_clusters_prom) / rmse_knn) * 100
mejora_r2 = ((r2_clusters_prom - r2_knn) / abs(r2_knn)) * 100

print(f"\nüìà MEJORAS AL USAR CLUSTERING:")
print(f"   ‚Ä¢ MAE:  {mejora_mae:+.2f}% ({'mejor' if mejora_mae > 0 else 'peor'})")
print(f"   ‚Ä¢ RMSE: {mejora_rmse:+.2f}% ({'mejor' if mejora_rmse > 0 else 'peor'})")
print(f"   ‚Ä¢ R¬≤:   {mejora_r2:+.2f}% ({'mejor' if mejora_r2 > 0 else 'peor'})")

# Visualizaci√≥n comparativa
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Panel 1: M√©tricas por cluster
ax = axes[0]
x_pos = list(resultados_clusters.keys())
maes = [resultados_clusters[i]['mae'] for i in x_pos]
colors = plt.cm.viridis(np.linspace(0, 1, len(x_pos)))
bars = ax.bar(x_pos, maes, color=colors, alpha=0.7, edgecolor='k')
ax.axhline(mae_knn, color='red', linestyle='--', linewidth=2,
          label=f'MAE Global: {mae_knn:.2f}')
ax.set_xlabel('Cluster', fontsize=12)
ax.set_ylabel('MAE', fontsize=12)
ax.set_title('MAE por Cluster vs Global', fontweight='bold', fontsize=14)
ax.legend(fontsize=11)
ax.grid(alpha=0.3, axis='y')

# Panel 2: Comparaci√≥n de m√©todos
ax = axes[1]
metodos = ['Global', 'Por Cluster']
maes_comp = [mae_knn, mae_clusters_prom]
rmses_comp = [rmse_knn, rmse_clusters_prom]
x_pos = np.arange(len(metodos))
width = 0.35
bars1 = ax.bar(x_pos - width/2, maes_comp, width, label='MAE',
              color='steelblue', alpha=0.7, edgecolor='k')
bars2 = ax.bar(x_pos + width/2, rmses_comp, width, label='RMSE',
              color='coral', alpha=0.7, edgecolor='k')
ax.set_xlabel('M√©todo', fontsize=12)
ax.set_ylabel('Error', fontsize=12)
ax.set_title('Comparaci√≥n de Errores', fontweight='bold', fontsize=14)
ax.set_xticks(x_pos)
ax.set_xticklabels(metodos)
ax.legend(fontsize=11)
ax.grid(alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

#%%
# ============================================================
# PASO 8: VISUALIZACI√ìN ESPACIAL DE PREDICCIONES
# ============================================================
print("\n" + "="*70)
print("üó∫Ô∏è  PASO 7: VISUALIZACI√ìN ESPACIAL")
print("="*70)

print("\nüìå Visualizando predicciones en el espacio...")

fig, axes = plt.subplots(2, 2, figsize=(16, 14))

# Panel 1: Datos de test reales
ax = axes[0, 0]
scatter = ax.scatter(x_test, z_test, c=attr_test, cmap='RdYlBu_r',
                    s=100, alpha=0.8, edgecolors='k', linewidth=0.8)
ax.set_title('Valores REALES (Test)', fontweight='bold', fontsize=13)
ax.set_xlabel('X (midx)', fontsize=11)
ax.set_ylabel('Z (midz)', fontsize=11)
ax.grid(alpha=0.3)
plt.colorbar(scatter, ax=ax, label='starkey_min')

# Panel 2: Predicciones globales
ax = axes[0, 1]
scatter = ax.scatter(x_test, z_test, c=pred_knn, cmap='RdYlBu_r',
                    s=100, alpha=0.8, edgecolors='k', linewidth=0.8)
ax.set_title(f'Predicciones GLOBALES\nRMSE={rmse_knn:.2f}',
            fontweight='bold', fontsize=13)
ax.set_xlabel('X (midx)', fontsize=11)
ax.set_ylabel('Z (midz)', fontsize=11)
ax.grid(alpha=0.3)
plt.colorbar(scatter, ax=ax, label='Predicho')

# Panel 3: Clusters en espacio
ax = axes[1, 0]
scatter = ax.scatter(x_test, z_test, c=clusters_test, cmap='viridis',
                    s=100, alpha=0.8, edgecolors='k', linewidth=0.8)
ax.set_title('Clusters de Test', fontweight='bold', fontsize=13)
ax.set_xlabel('X (midx)', fontsize=11)
ax.set_ylabel('Z (midz)', fontsize=11)
ax.grid(alpha=0.3)
plt.colorbar(scatter, ax=ax, label='Cluster')

# Panel 4: Errores espaciales
ax = axes[1, 1]
errores_espaciales = attr_test - pred_knn
scatter = ax.scatter(x_test, z_test, c=errores_espaciales, cmap='RdYlGn_r',
                    s=100, alpha=0.8, edgecolors='k', linewidth=0.8,
                    vmin=-abs(errores_espaciales).max(),
                    vmax=abs(errores_espaciales).max())
ax.set_title('Errores de Predicci√≥n\n(Rojo=Subestimado, Verde=Sobreestimado)',
            fontweight='bold', fontsize=12)
ax.set_xlabel('X (midx)', fontsize=11)
ax.set_ylabel('Z (midz)', fontsize=11)
ax.grid(alpha=0.3)
plt.colorbar(scatter, ax=ax, label='Error')

plt.tight_layout()
plt.show()

#%%
# ============================================================
# PASO 9: RESUMEN Y CONCLUSIONES
# ============================================================
print("\n" + "="*70)
print("üìù RESUMEN Y CONCLUSIONES")
print("="*70)

print(f"""
‚úÖ PIPELINE COMPLETADO CON √âXITO

üîπ DATOS PROCESADOS:
   ‚Ä¢ Total de puntos: {len(df)}
   ‚Ä¢ Entrenamiento: {len(x_train)} ({len(x_train)/len(x)*100:.1f}%)
   ‚Ä¢ Test: {len(x_test)} ({len(x_test)/len(x)*100:.1f}%)

üîπ CLUSTERIZACI√ìN:
   ‚Ä¢ N√∫mero de clusters: {n_clusters}
   ‚Ä¢ Peso espacial: {w_spatial}
   ‚Ä¢ Std promedio: {metricas['std_prom']:.2f}

üîπ ESTIMACI√ìN GLOBAL (KNN):
   ‚Ä¢ Vecinos (k): {n_neighbors}
   ‚Ä¢ MAE: {mae_knn:.3f}
   ‚Ä¢ RMSE: {rmse_knn:.3f}
   ‚Ä¢ R¬≤: {r2_knn:.3f}

üîπ ESTIMACI√ìN POR CLUSTER:
   ‚Ä¢ Modelos entrenados: {len(resultados_clusters)}
   ‚Ä¢ MAE promedio: {mae_clusters_prom:.3f}
   ‚Ä¢ RMSE promedio: {rmse_clusters_prom:.3f}
   ‚Ä¢ R¬≤ promedio: {r2_clusters_prom:.3f}

üîπ MEJORA AL USAR CLUSTERING:
   ‚Ä¢ MAE: {mejora_mae:+.2f}%
   ‚Ä¢ RMSE: {mejora_rmse:+.2f}%
   ‚Ä¢ R¬≤: {mejora_r2:+.2f}%

üìå INTERPRETACI√ìN:
   
   1. CLUSTERIZACI√ìN:
      La clusterizaci√≥n agrupa los datos en {n_clusters} dominios espaciales
      que son homog√©neos en t√©rminos del atributo starkey_min.
   
   2. ESTIMACI√ìN:
      KNN usa los {n_neighbors} vecinos m√°s cercanos para predecir valores.
      Al estimar dentro de cada cluster, respetamos la homogeneidad local.
   
   3. RESULTADOS:
      {'La estimaci√≥n por cluster MEJORA' if mejora_mae > 0 else 'La estimaci√≥n global es MEJOR'}
      los resultados, reduciendo el error en {abs(mejora_mae):.1f}%.
      
      Esto indica que {'los dominios tienen caracter√≠sticas distintas' if mejora_mae > 0 else 'los datos son bastante homog√©neos'}
      {'y se benefician de modelos especializados.' if mejora_mae > 0 else 'y un modelo global es suficiente.'}

üéØ PR√ìXIMOS PASOS:
   1. Probar diferentes valores de k (vecinos)
   2. Experimentar con otros n√∫meros de clusters
   3. Comparar con otros m√©todos (IDW, Kriging)
   4. Generar estimaciones en grilla completa
   5. Cuantificar incertidumbre de las predicciones
""")

print("\n" + "="*70)
print("‚úÖ AN√ÅLISIS COMPLETADO")
print("="*70)

#%%
print("\nüéâ ¬°Script ejecutado exitosamente!")
print("üìä Todos los resultados han sido generados y visualizados.")